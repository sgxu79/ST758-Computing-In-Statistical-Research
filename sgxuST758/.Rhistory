g_norm = norm(gradf_new,type='2')
f_cha = abs(f_new-f_old)
f_cha
f_rel_cha = abs(f_cha/f_old)
b_rel_cha = norm((beta_new-beta_old),type="2")/norm(beta_old,type="2")
result[["iter_val"]] = c(result[["iter_val"]],beta_new)
result[["fun_val"]] = c(result[["fun_val"]],f_new)
result[["norm_g"]] = c(result[["norm_g"]],g_norm)
result[["rel_fun"]] = c(result[["rel_fun"]],f_rel_cha)
result[["rel_iter"]] = c(result[["rel_iter"]],b_rel_cha)
count = count + 1
beta_old = beta_new
}
return(result)
}
fx_logistic_wrapper <- function(beta) {
return(fx_logistic(y, X, beta,lambda))
}
gradf_logistic_wrapper <- function(beta) {
return(gradf_logistic(y, X, beta,lambda))
}
fsum = gradient_descent_fixed(fx_logistic_wrapper, gradf_logistic_wrapper,
t, beta0, max_iter=1e4, tol=0)
print(f_cha)
library(sgxuST758)
library(sgxuST758)
fsum = gradient_descent_fixed(fx_logistic_wrapper, gradf_logistic_wrapper,
t, beta0, max_iter=1e4, tol=0)
gradient_descent_fixed(fx_logistic_wrapper, gradf_logistic_wrapper,
t, beta0, max_iter=1e4, tol=0)
n <- 100
p <- 2
X <- matrix(rnorm(n*p),n,p)
beta0 <- matrix(rnorm(p),p,1)
y <- (runif(n) <= plogis(X%*%beta0)) + 0
L = (norm(X,type="2")^2)/4
t = 1/(2*L)
lambda = 0
fx_logistic_wrapper <- function(beta) {
return(fx_logistic(y, X, beta,lambda))
}
gradf_logistic_wrapper <- function(beta) {
return(gradf_logistic(y, X, beta,lambda))
}
x0 = beta0
max_iter = 100
tol = 0
beta_old = x0
gradf_new = gradf(x0)
graf <- gradf_logistic_wrapper()
graf <- gradf_logistic_wrapper
graf(x0)
fx = fx_logistic_wrapper
f_new = fx(x0)
f_new
count = 0
f_cha = 1
result = list("iter_val"=c(),"fun_val"=c(),"norm_g"=c(),"rel_fun"=c(),"rel_iter"=c())
while((count < max_iter) && (f_cha > tol)){
beta_new = gradient_step(gradf_new, t, beta_old)
f_old = f_new
f_new =  fx(beta_new)
gradf_new = gradf(beta_new)
g_norm = norm(gradf_new,type='2')
f_cha = abs(f_new-f_old)
print(f_cha)
f_rel_cha = abs(f_cha/f_old)
b_rel_cha = norm((beta_new-beta_old),type="2")/norm(beta_old,type="2")
result[["iter_val"]] = c(result[["iter_val"]],beta_new)
result[["fun_val"]] = c(result[["fun_val"]],f_new)
result[["norm_g"]] = c(result[["norm_g"]],g_norm)
result[["rel_fun"]] = c(result[["rel_fun"]],f_rel_cha)
result[["rel_iter"]] = c(result[["rel_iter"]],b_rel_cha)
count = count + 1
beta_old = beta_new
}
gradf_new = gradf(x0)
graf()
graf(x0)
gradf_new = gradf(x0)
fsum = gradient_descent_fixed(fx_logistic_wrapper, gradf_logistic_wrapper,
t, beta0, max_iter=10, tol=0)
iter_num = length(fsum$fun_val)
iter_num
fsum = gradient_descent_fixed(fx_logistic_wrapper, gradf_logistic_wrapper,
t, beta0, max_iter=100, tol=0)
iter_num = length(fsum$fun_val)
iter_num
fsum = gradient_descent_fixed(fx_logistic_wrapper, gradf_logistic_wrapper,
t, beta0, max_iter=100, tol=-1)
iter_num = length(fsum$fun_val)
iter_num
fsum = gradient_descent_fixed(fx_logistic_wrapper, gradf_logistic_wrapper,
t, beta0, max_iter=1e4, tol=-1)
fsum = gradient_descent_fixed(fx_logistic_wrapper, gradf_logistic_wrapper,
t, beta0, max_iter=1e4, tol=-1)
l_beta = fsum$fun_val
l_beta = fsum$fun_val
l_change = l_beta - l_beta[1e4]
l_beta
plot(1:1e4,l_change,type='l',main = TeX('$l(\\beta_k)-\\l(\\beta_{10000})$ vs iteration  number'),
ylab=TeX('$l(\\beta_k)-l(\\beta_{10000})$'),xlab = 'Iteration number')
plot(c(1:100),l_change[1:100],type='l',main = "Zoomed in version",
ylab=TeX('$l(\\beta_k)-l(\\beta_{10000})$'),xlab = 'Iteration number')
plot(c(1:60),l_change[1:60],type='l',main = "Zoomed in version",
ylab=TeX('$l(\\beta_k)-l(\\beta_{10000})$'),xlab = 'Iteration number')
plot(c(1:40),l_change[1:40],type='l',main = "Zoomed in version",
ylab=TeX('$l(\\beta_k)-l(\\beta_{10000})$'),xlab = 'Iteration number')
plot(c(1:20),l_change[1:20],type='l',main = "Zoomed in version",
ylab=TeX('$l(\\beta_k)-l(\\beta_{10000})$'),xlab = 'Iteration number')
lambda = 10
fsum = gradient_descent_fixed(fx_logistic_wrapper, gradf_logistic_wrapper,
t, beta0, max_iter=1e4, tol=-1)
l_beta = fsum$fun_val
l_change = l_beta - l_beta[1e4]
plot(1:1e4,l_change,type='l',main = TeX('$l_{\\lambda}(\\beta_k)-\\l_{\\lambda}(\\beta_{10000})$ vs iteration  number'),
ylab=TeX('$l_{\\lambda}(\\beta_k)-l_{\\lambda}(\\beta_{10000})$'),xlab = 'Iteration number')
plot(c(1:40),l_change[1:40],type='l',main = 'Zoomed in version', ylab=Tex('$l_{\\lambda}(\\beta_k)-l_{\\lambda}(\\beta_10000)'),xlab = 'Iteration number')
plot(c(1:40),l_change[1:40],type='l',main = 'Zoomed in version', ylab=TeX('$l_{\\lambda}(\\beta_k)-l_{\\lambda}(\\beta_10000)'),xlab = 'Iteration number')
plot(c(1:20),l_change[1:20],type='l',main = 'Zoomed in version', ylab=TeX('$l_{\\lambda}(\\beta_k)-l_{\\lambda}(\\beta_10000)'),xlab = 'Iteration number')
plot(c(1:10),l_change[1:10],type='l',main = 'Zoomed in version', ylab=TeX('$l_{\\lambda}(\\beta_k)-l_{\\lambda}(\\beta_10000)'),xlab = 'Iteration number')
plot(c(1:40),l_change[1:40],type='l',main = 'Zoomed in version', ylab=TeX('$l_{\\lambda}(\\beta_k)-l_{\\lambda}(\\beta_10000)'),xlab = 'Iteration number')
effn = sum(l_beta<1e-4)
effn
l_beta = fsum$fun_val
effn = sum(l_beta<1e-4)
effn
length(l_beta)
length(l_beta)[1:10]
lambda = 10
fsum = gradient_descent_fixed(fx_logistic_wrapper, gradf_logistic_wrapper,
t, beta0, max_iter=1e4, tol=-1)
l_beta = fsum$fun_val
l_beta[1:10]
l_change = l_beta - l_beta[1e4]
effn = sum(l_beta<1e-4)
effn
l_beta[1:5]
effn = sum(l_change<1e-4)
effn
l_change[1:10]
l_change[1:10]<1e-4
sum(l_change[1:10]<1e-4)
effn = sum(l_change>1e-4)
effn
lambda = 10
iter = 1e2
set.seed(12345)
## Data set 1
n <- 200
p <- 400
X1 <- matrix(rnorm(n*p),n,p)
beta01 <- matrix(rnorm(p),p,1)
y1 <- (runif(n) <= plogis(X1%*%beta01)) + 0
X = X1
y = y1
beta0 = beta01
time1 = system.time({logistic_ridge_newton(X, y, beta0, lambda, naive=TRUE,
max_iter=iter, tol=1e-3)})[3]
logistic_ridge_newton <- function(X, y, beta, lambda=0, naive=TRUE, max_iter=1e2, tol=1e-3) {
t = 1
beta_new = beta
count = 0
HSND = 1
while((count<max_iter) & (HSND>tol)){
beta_old = beta_new
gradf = gradf_logistic(y, X, beta_old,lambda)
if(naive == 0) Newton_step = newton_step_smw(X, y, beta_old, gradf, lambda)
else Newton_step = newton_step_naive(X, y, beta_old, gradf, lambda)
HSND = t(gradf)%*%Newton_step/2
step_size = backtrack_descent(fx_logistic_wrapper, beta_old, t, gradf, Newton_step, alpha=0.5, 0.9)
beta_new = beta_old - step_size*Newton_step
count = count + 1
}
return(beta_new)
}
time1 = system.time({logistic_ridge_newton(X, y, beta0, lambda, naive=TRUE,
max_iter=iter, tol=1e-3)})[3]
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
library(sgxuST758)
if(!require("latex2exp")) install.packages("latex2exp")
set.seed(12345)
n <- 100
p <- 2
X <- matrix(rnorm(n*p),n,p)
beta0 <- matrix(rnorm(p),p,1)
y <- (runif(n) <= plogis(X%*%beta0)) + 0
L = (norm(X,type="2")^2)/4
t = 1/(2*L)
lambda = 0
fx_logistic_wrapper <- function(beta) {
return(fx_logistic(y, X, beta,lambda))
}
gradf_logistic_wrapper <- function(beta) {
return(gradf_logistic(y, X, beta,lambda))
}
fsum = gradient_descent_fixed(fx_logistic_wrapper, gradf_logistic_wrapper,
t, beta0, max_iter=1e4, tol=-1)
l_beta = fsum$fun_val
l_change = l_beta - l_beta[1e4]
effn = sum(l_change>1e-4)
plot(1:1e4,l_change,type='l',main = TeX('$l(\\beta_k)-\\l(\\beta_{10000})$ vs iteration  number'),
ylab=TeX('$l(\\beta_k)-l(\\beta_{10000})$'),xlab = 'Iteration number')
plot(c(1:40),l_change[1:40],type='l',main = "Zoomed in version",
ylab=TeX('$l(\\beta_k)-l(\\beta_{10000})$'),xlab = 'Iteration number')
lambda = 10
iter = 1e2
set.seed(12345)
## Data set 1
n <- 200
p <- 400
X1 <- matrix(rnorm(n*p),n,p)
beta01 <- matrix(rnorm(p),p,1)
y1 <- (runif(n) <= plogis(X1%*%beta01)) + 0
X = X1
y = y1
beta0 = beta01
time1 = system.time({logistic_ridge_newton(X, y, beta0, lambda, naive=TRUE,
max_iter=iter, tol=1e-3)})[3]
library(sgxuST758)
lambda = 10
iter = 1e2
set.seed(12345)
## Data set 1
n <- 200
p <- 400
X1 <- matrix(rnorm(n*p),n,p)
beta01 <- matrix(rnorm(p),p,1)
y1 <- (runif(n) <= plogis(X1%*%beta01)) + 0
X = X1
y = y1
beta0 = beta01
time1 = system.time({logistic_ridge_newton(X, y, beta0, lambda, naive=TRUE,
max_iter=iter, tol=1e-3)})[3]
library(sgxuST758)
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
library(sgxuST758)
if(!require("latex2exp")) install.packages("latex2exp")
time1 = system.time({logistic_ridge_newton(X, y, beta0, lambda, naive=TRUE,
max_iter=iter, tol=1e-3)})[3]
library(sgxuST758)
time1 = system.time({logistic_ridge_newton(X, y, beta0, lambda, naive=TRUE,
max_iter=iter, tol=1e-3)})[3]
lambda = 10
iter = 1e2
set.seed(12345)
## Data set 1
n <- 200
p <- 400
X1 <- matrix(rnorm(n*p),n,p)
beta01 <- matrix(rnorm(p),p,1)
y1 <- (runif(n) <= plogis(X1%*%beta01)) + 0
X = X1
y = y1
beta0 = beta01
time1 = system.time({logistic_ridge_newton(X, y, beta0, lambda, naive=TRUE,
max_iter=iter, tol=1e-3)})[3]
newton_step_naive <- function(X, y, beta, g, lambda) {
p = ncol(X)
W = diag(as.vector(plogis(X%*%beta)))
A = lambda*diag(p) + t(X)%*%W%*%X
R = chol(A)
u = forwardsolve(t(R), g)
Newton_step = backsolve(R, u)
return(Newton_step)
}
time1 = system.time({logistic_ridge_newton(X, y, beta0, lambda, naive=TRUE,
max_iter=iter, tol=1e-3)})[3]
#' Gradient Step
#'
#' @param gradf handle to function that returns gradient of objective function
#' @param x current parameter estimate
#' @param t step-size
#' @export
gradient_step <- function(gradf, t, x) {
x_star = x - t*gradf
return(x_star)
}
#' Gradient Descent (Fixed Step-Size)
#'
#' @param fx handle to function that returns objective function values
#' @param gradf handle to function that returns gradient of objective function
#' @param x0 initial parameter estimate
#' @param t step-size
#' @param max_iter maximum number of iterations
#' @param tol convergence tolerance
#' @export
gradient_descent_fixed <- function(fx, gradf, t, x0, max_iter=1e2, tol=1e-3) {
beta_old = x0
gradf_new = gradf(x0)
f_new = fx(x0)
count = 0
f_cha = 1
result = list("iter_val"=c(),"fun_val"=c(),"norm_g"=c(),"rel_fun"=c(),"rel_iter"=c())
while((count < max_iter) && (f_cha > tol)){
beta_new = gradient_step(gradf_new, t, beta_old)
f_old = f_new
f_new =  fx(beta_new)
gradf_new = gradf(beta_new)
g_norm = norm(gradf_new,type='2')
f_cha = abs(f_new-f_old)
f_rel_cha = abs(f_cha/f_old)
b_rel_cha = norm((beta_new-beta_old),type="2")/norm(beta_old,type="2")
result[["iter_val"]] = c(result[["iter_val"]],beta_new)
result[["fun_val"]] = c(result[["fun_val"]],f_new)
result[["norm_g"]] = c(result[["norm_g"]],g_norm)
result[["rel_fun"]] = c(result[["rel_fun"]],f_rel_cha)
result[["rel_iter"]] = c(result[["rel_iter"]],b_rel_cha)
count = count + 1
beta_old = beta_new
}
return(result)
}
#' Objective Function for Logistic Regression
#'
#' @param y binary response
#' @param X design matrix
#' @param beta regression coefficient vector
#' @param lambda regularization parameter
#' @export
fx_logistic <- function(y, X, beta, lambda=0) {
fx = sum(log(1 + exp(X%*%beta))) - sum(y*(X%*%beta)) + (lambda/2)*sum(beta*beta)
return(fx)
}
#' Gradient for Logistic Regession
#'
#' @param y binary response
#' @param X design matrix
#' @param beta regression coefficient vector
#' @param lambda regularization parameter
#' @export
gradf_logistic <- function(y, X, beta, lambda=0) {
gradf = 0
for(i in 1:length(y)){
gradf = exp(sum(X[i,]*beta))*X[i,]/(1+exp(sum(X[i,]*beta))) -y[i]*X[i,] + gradf
}
gradf = gradf + lambda*beta
return(gradf)
}
#' Compute Newton Step (Naive) for logistic ridge regression
#'
#' @param X Design matrix
#' @param y Binary response vector
#' @param beta Current regression vector estimate
#' @param g Gradient vector
#' @param lambda Regularization parameter
newton_step_naive <- function(X, y, beta, g, lambda) {
p = ncol(X)
W = diag(as.vector(plogis(X%*%beta)))
A = lambda*diag(p) + t(X)%*%W%*%X
R = chol(A)
u = forwardsolve(t(R), g)
Newton_step = backsolve(R, u)
return(Newton_step)
}
#' Compute Newton Step (Sherman-Morrison-Woodbury) for logistic ridge regression
#'
#' @param X Design matrix
#' @param y Binary response vector
#' @param beta Current regression vector estimate
#' @param g Gradient vector
#' @param lambda Regularization parameter
newton_step_smw <- function(X, y, beta, g, lambda) {
p = ncol(X)
W_inv = diag(as.vector(1/plogis(X%*%beta)))
cma = chol(W_inv + X%*%t(X)/lambda)
cma_inv = chol2inv(cma)
Hessian_inv = diag(p)/lambda - t(X)%*%cma_inv%*%X/lambda^2
Newton_step = Hessian_inv%*%g
return(Newton_step)
}
#' Backtracking for steepest descent
#'
#' @param fx handle to function that returns objective function values
#' @param x current parameter estimate
#' @param t current step-size
#' @param df the value of the gradient of objective function evaluated at the current x
#' @param d descent direction vector
#' @param alpha the backtracking parameter
#' @param beta the decrementing multiplier
backtrack_descent <- function(fx, x, t, df, d, alpha=0.5, beta=0.9) {
ss = t
f_new = fx(x)
while(f_new > (fx(x)-alpha*ss*t(df)%*%d)){
ss = beta*ss
f_new = fx(x-ss*d)
}
return(ss)
}
#' Damped Newton's Method for Fitting Ridge Logistic Regression
#'
#' @param y Binary response
#' @param X Design matrix
#' @param beta Initial regression coefficient vector
#' @param lambda regularization parameter
#' @param naive Boolean variable; TRUE if using Cholesky on the Hessian
#' @param max_iter maximum number of iterations
#' @param tol convergence tolerance
logistic_ridge_newton <- function(X, y, beta, lambda=0, naive=TRUE, max_iter=1e2, tol=1e-3) {
t = 1
beta_new = beta
count = 0
HSND = 1
while((count<max_iter) & (HSND>tol)){
beta_old = beta_new
gradf = gradf_logistic(y, X, beta_old,lambda)
if(naive == 0) Newton_step = newton_step_smw(X, y, beta_old, gradf, lambda)
else Newton_step = newton_step_naive(X, y, beta_old, gradf, lambda)
HSND = t(gradf)%*%Newton_step/2
step_size = backtrack_descent(fx_logistic_wrapper, beta_old, t, gradf, Newton_step, alpha=0.5, 0.9)
beta_new = beta_old - step_size*Newton_step
count = count + 1
}
return(beta_new)
}
time1 = system.time({logistic_ridge_newton(X, y, beta0, lambda, naive=TRUE,
max_iter=iter, tol=1e-3)})[3]
library(sgxuST758)
lambda = 10
iter = 1e2
set.seed(12345)
## Data set 1
n <- 200
p <- 400
X1 <- matrix(rnorm(n*p),n,p)
beta01 <- matrix(rnorm(p),p,1)
y1 <- (runif(n) <= plogis(X1%*%beta01)) + 0
X = X1
y = y1
beta0 = beta01
time1 = system.time({logistic_ridge_newton(X, y, beta0, lambda, naive=TRUE,
max_iter=iter, tol=1e-3)})[3]
## Data set 2
p <- 800
X2 <- matrix(rnorm(n*p),n,p)
beta02 <- matrix(rnorm(p),p,1)
y2 <- (runif(n) <= plogis(X2%*%beta02)) + 0
X = X2
y = y2
beta0 = beta02
time2 = system.time({logistic_ridge_newton(X, y, beta0, lambda, naive=TRUE,
max_iter=iter, tol=1e-3)})[3]
## Data set 3
p <- 1600
X3 <- matrix(rnorm(n*p),n,p)
beta03 <- matrix(rnorm(p),p,1)
y3 <- (runif(n) <= plogis(X3%*%beta03)) + 0
X = X3
y = y3
beta0 = beta03
time3 = system.time({logistic_ridge_newton(X, y, beta0, lambda, naive=TRUE,
max_iter=iter, tol=1e-3)})[3]
library(sgxuST758)
## Data set 1
X = X1
y = y1
beta0 = beta01
time4 = system.time({logistic_ridge_newton(X, y, beta0, lambda, naive=FALSE,
max_iter=iter, tol=1e-3)})[3]
## Data set 2
X = X2
y = y2
beta0 = beta02
time5 = system.time({logistic_ridge_newton(X, y, beta0, lambda, naive=FALSE,
max_iter=iter, tol=1e-3)})[3]
## Data set 3
X = X3
y = y3
beta0 = beta03
time6 = system.time({logistic_ridge_newton(X, y, beta0, lambda, naive=FALSE,
max_iter=iter, tol=1e-3)})[3]
time = c(time1,time2,time3,time4,time5,time6)
time
plot(c(400,800,1600),c(time1,time2,time3),type = 'p',pch = "o",col='red',
main = 'Run time against p',ylab='run time',xlab = 'p')
points(c(400,800,1600),c(time4,time5,time6),type = 'p',pch = "*",col='black')
legend("topleft",legend=c("naive","smw"), col=c("red","black"),pch = c("o","*"))
time = c(time1,time2,time3,time4,time5,time6)
time
plot(c(400,800,1600),time[1:3],type="l",lty=1,main = 'Run time against p',ylab='run time',xlab = 'p')
lines(c(400,800,1600),time[4:6],lty=2,col=2)
#plot(c(400,800,1600),c(time1,time2,time3),type = 'p',pch = "o",col='red',
#     main = 'Run time against p',ylab='run time',xlab = 'p')
#points(c(400,800,1600),c(time4,time5,time6),type = 'p',pch = "*",col='black')
legend("topleft",legend=c("Naive","SMW"), col=1:2,lty=1:2)
plot(c(400,800,1600),time[1:3],type=c("l","p"),lty=1,main = 'Run time against p',ylab='run time',xlab = 'p')
plot(c(400,800,1600),time[1:3],type=c("b"),lty=1,main = 'Run time against p',ylab='run time',xlab = 'p')
time = c(time1,time2,time3,time4,time5,time6)
time
plot(c(400,800,1600),time[1:3],type=c("b"),lty=1,main = 'Run time against p',ylab='run time',xlab = 'p')
lines(c(400,800,1600),time[4:6],lty=2,col=2)
points(c(400,800,1600),time[4:6],type = 'p',pch = "o",col='black')
legend("topleft",legend=c("Naive","SMW"), col=1:2,lty=1:2)
time = c(time1,time2,time3,time4,time5,time6)
time
plot(c(400,800,1600),time[1:3],type=c("b"),lty=1,main = 'Run time against p',ylab='run time',xlab = 'p')
lines(c(400,800,1600),time[4:6],lty=2,col=2)
points(c(400,800,1600),time[4:6],type = 'p',pch = "o",col='red')
legend("topleft",legend=c("Naive","SMW"), col=1:2,lty=1:2)
time = c(time1,time2,time3,time4,time5,time6)
time
plot(c(400,800,1600),time[1:3],type=c("b"),lty=1,main = 'Run time against p',ylab='run time',xlab = 'p')
lines(c(400,800,1600),time[4:6],lty=2,col=2)
points(c(400,800,1600),time[4:6],type = 'p',pch = 20,col='red')
legend("topleft",legend=c("Naive","SMW"), col=1:2,lty=1:2)
time = c(time1,time2,time3,time4,time5,time6)
time
plot(c(400,800,1600),time[1:3],type=c("b"),lty=1,main = 'Run time against p',ylab='run time',xlab = 'p')
lines(c(400,800,1600),time[4:6],lty=2,col=2)
points(c(400,800,1600),time[4:6],type = 'p',pch = 20,col='red')
legend("topleft",legend=c("Naive","SMW"), col=1:2,pch=c(1,20),lty=1:2)
library(sgxuST758)
